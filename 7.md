# 第七章 到达哲学

本章的目标是开发一个 Web 爬虫，它测试了第 6.1 节中提到的“到达哲学”猜想。

## 7.1 起步

在本书的仓库中，你将找到一些帮助你起步的代码：

+   `WikiNodeExample.java`包含前一章的代码，展示了 DOM 树中深度优先搜索（DFS）的递归和迭代实现。
+   `WikiNodeIterable.java`包含`Iterable`类，用于遍历 DOM 树。我将在下一节中解释这段代码。
+   `WikiFetcher.java`包含一个工具类，使用`jsoup`从维基百科下载页面。为了帮助你遵守维基百科的服务条款，此类限制了你下载页面的速度；如果你每秒请求许多页，在下载下一页之前会休眠一段时间。
+   `WikiPhilosophy.java`包含你为此练习编写的代码的大纲。我们将在下面进行说明。

你还会发现 Ant 构建文件`build.xml`。如果你运行`ant WikiPhilosophy`，它将运行一些简单的启动代码。

## 7.2 可迭代对象和迭代器

在前一章中，我展示了迭代式深度优先搜索（DFS），并且认为与递归版本相比，迭代版本的优点在于，它更容易包装在`Iterator`对象中。在本节中，我们将看到如何实现它。

如果你不熟悉`Iterator`和`Iterable`接口，你可以阅读 <http://thinkdast.com/iterator> 和 <http://thinkdast.com/iterable>。

看看`WikiNodeIterable.java`的内容。外层的类`WikiNodeIterable`实现`Iterable<Node>`接口，所以我们可以在一个`for`循环中使用它：

```java
Node root = ...
Iterable<Node> iter = new WikiNodeIterable(root);
for (Node node: iter) {
    visit(node);
}
```

其中`root`是我们想要遍历的树的根节点，并且`visit`是一个方法，当我们“访问”`Node`时，做任何我们想要的事情。

`WikiNodeIterable`的实现遵循以下惯例：

+   构造函数接受并存储根`Node`的引用。
+   `iterator`方法创建一个返回一个`Iterator`对象。

这是它的样子：

```java
public class WikiNodeIterable implements Iterable<Node> {

    private Node root;

    public WikiNodeIterable(Node root) {
        this.root = root;
    }

    @Override
    public Iterator<Node> iterator() {
        return new WikiNodeIterator(root);
    }
}
```

内层的类`WikiNodeIterator`，执行所有实际工作。

```java
private class WikiNodeIterator implements Iterator<Node> {

    Deque<Node> stack;

    public WikiNodeIterator(Node node) {
        stack = new ArrayDeque<Node>();
        stack.push(root);
    }

    @Override
    public boolean hasNext() {
        return !stack.isEmpty();
    }

    @Override
    public Node next() {
        if (stack.isEmpty()) {
            throw new NoSuchElementException();
        }

        Node node = stack.pop();
        List<Node> nodes = new ArrayList<Node>(node.childNodes());
        Collections.reverse(nodes);
        for (Node child: nodes) {
            stack.push(child);
        }
        return node;
    }
}
```

该代码与 DFS 的迭代版本几乎相同，但现在分为三个方法：

+   构造函数初始化栈（使用一个`ArrayDeque`实现）并将根节点压入这个栈。
+   `isEmpty`检查栈是否为空。
+   `next`从`Node`栈中弹出下一个节点，按相反的顺序压入子节点，并返回弹出的`Node`。如果有人在空`Iterator`上调用`next`，则会抛出异常。

可能不明显的是，值得使用两个类和五个方法，来重写一个完美的方法。但是现在我们已经完成了，在需要`Iterable`的任何地方，我们可以使用`WikiNodeIterable`，这使得它的语法整洁，易于将迭代逻辑（DFS）与我们对节点的处理分开。

## 7.3 `WikiFetcher`

编写 Web 爬虫时，很容易下载太多页面，这可能会违反你要下载的服务器的服务条款。为了帮助你避免这种情况，我提供了一个`WikiFetcher`类，它可以做两件事情：

+   它封装了我们在上一章中介绍的代码，用于从维基百科下载页面，解析 HTML 以及选择内容文本。
+   它测量请求之间的时间，如果我们在请求之间没有足够的时间，它将休眠直到经过了合理的间隔。默认情况下，间隔为`1`秒。

这里是`WikiFetcher`的定义：

```java
public class WikiFetcher {
    private long lastRequestTime = -1;
    private long minInterval = 1000;

    /**
     * Fetches and parses a URL string, 
     * returning a list of paragraph elements.
     *
     * @param url
     * @return
     * @throws IOException
     */
    public Elements fetchWikipedia(String url) throws IOException {
        sleepIfNeeded();

        Connection conn = Jsoup.connect(url);
        Document doc = conn.get();
        Element content = doc.getElementById("mw-content-text");
        Elements paragraphs = content.select("p");
        return paragraphs;
    }

    private void sleepIfNeeded() {
        if (lastRequestTime != -1) {
            long currentTime = System.currentTimeMillis();
            long nextRequestTime = lastRequestTime + minInterval;
            if (currentTime < nextRequestTime) {
                try {
                    Thread.sleep(nextRequestTime - currentTime);
                } catch (InterruptedException e) {
                    System.err.println(
                        "Warning: sleep interrupted in fetchWikipedia.");
                }
            }
        }
        lastRequestTime = System.currentTimeMillis();
    }
}
```

唯一的公共方法是`fetchWikipedia`，接收`String`形式的 URL，并返回一个`Elements`集合，该集合包含的一个 DOM 元素表示内容文本中每个段落。这段代码应该很熟悉了。

新的代码是`sleepIfNeeded`，它检查自上次请求以来的时间，如果经过的时间小于`minInterval`（毫秒），则休眠。

这就是`WikiFetcher`全部。这是一个演示如何使用它的例子：

```java
WikiFetcher wf = new WikiFetcher();

for (String url: urlList) {
    Elements paragraphs = wf.fetchWikipedia(url);
    processParagraphs(paragraphs);
}
```

在这个例子中，我们假设`urlList`是一个`String`的集合 ，并且`processParagraphs`是一个方法，对`Elements`做一些事情，它由`fetchWikipedia`返回。

此示例展示了一些重要的东西：你应该创建一个`WikiFetcher`对象并使用它来处理所有请求。如果有多个`WikiFetcher`的实例，则它们不会确保请求之间的最小间隔。

注意：我的`WikiFetcher`实现很简单，但是通过创建多个实例，人们很容易误用它。你可以通过制作`WikiFetcher`“单例” 来避免这个问题，你可以阅读 <http://thinkdast.com/singleton>。

